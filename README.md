# Generative AI Engineering and Fine‑Tuning Transformers – IBM

This repository contains code, notebooks, and notes developed during the [Generative AI Engineering and Fine‑Tuning Transformers](https://www.coursera.org/learn/generative-ai-engineering-and-fine-tuning-transformers) course offered by **IBM** through **Coursera**.

<p align="center">
  <img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png" title="IBM Skills Network" width="400" />
</p>

## About the Course

This is an **intermediate-level** course (~8 hours) designed to build job-ready skills for working with transformer-based LLMs. You’ll learn to perform parameter-efficient fine-tuning (PEFT) techniques such as **LoRA** and **QLoRA**, use pre-trained models, and fine-tune them using **Hugging Face** and **PyTorch** :contentReference[oaicite:1]{index=1}.

What you'll learn:

- Loading and running inference on pre-trained transformer models
- Pre-training and fine-tuning LLMs with **PyTorch** and **Hugging Face**
- Advanced fine-tuning methods: **PEFT**, **LoRA**, **QLoRA**
- Ethical and performance considerations when fine-tuning models

## Repository Structure

Organized by modules following the course outline:
```
📁 Repository structure
├── 📁 Module 1 – Transformers & Fine-Tuning Basics
│ ├── 📝 Lab: Loading Models and Running Inference
│ ├── 📝 Lab: Pre-training and Fine-tuning LLMs
├── 📁 Module 2 – Parameter-Efficient Fine-Tuning
│ ├── 📝 Lab: Using LoRA for Efficient Adaptation
│ ├── 📝 Lab: Quantized LoRA (QLoRA)
├── README.md
```

## Technologies Used

- **Python**
- **PyTorch**
- **Hugging Face Transformers**
- **Jupyter Notebooks**

## How to Use

1. Clone the repository:

```bash
git clone https://github.com/phaa/ibm-fine-tuning-transformers.git
cd ibm-fine-tuning-transformers/
```

2. Activate the virtual environment (conda or venv):
   ```bash
   conda activate ibmenv
   ```
3. Run the notebooks in Jupyter lab:  
   ```bash
   jupyter lab
   ```
*Each notebook has a cell to install the necessary dependencies.* 

## Contributions  
This repository is a record of the course learning, but suggestions and improvements are always welcome!

